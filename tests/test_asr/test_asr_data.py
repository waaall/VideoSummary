"""ASRData æ ¸å¿ƒåŠŸèƒ½æµ‹è¯• - ä¸¥æ ¼è¾¹ç¼˜ç”¨ä¾‹"""

import tempfile
from pathlib import Path

import pytest

from app.core.asr.asr_data import ASRData, ASRDataSeg


class TestASRDataSegEdgeCases:
    """æµ‹è¯• ASRDataSeg è¾¹ç¼˜æƒ…å†µ"""

    def test_zero_duration_segment(self):
        """æµ‹è¯•é›¶æ—¶é•¿å­—å¹•æ®µ"""
        seg = ASRDataSeg("Instant", 1000, 1000)
        assert seg.start_time == seg.end_time
        timestamp = seg.to_srt_ts()
        assert timestamp == "00:00:01,000 --> 00:00:01,000"

    def test_negative_duration(self):
        """æµ‹è¯•å€’åºæ—¶é—´æˆ³(start > end)"""
        seg = ASRDataSeg("Reversed", 2000, 1000)
        assert seg.start_time > seg.end_time  # ä¸åº”è‡ªåŠ¨ä¿®æ­£

    def test_very_long_timestamp(self):
        """æµ‹è¯•è¶…é•¿æ—¶é—´æˆ³(è¶…è¿‡24å°æ—¶)"""
        seg = ASRDataSeg("Long", 90000000, 90001000)  # 25å°æ—¶
        timestamp = seg.to_srt_ts()
        assert "25:00:00,000" in timestamp

    def test_unicode_text_extreme(self):
        """æµ‹è¯•æç«¯Unicodeæ–‡æœ¬"""
        # Emoji + ä¸­æ–‡ + æ—¥æ–‡ + éŸ©æ–‡ + é˜¿æ‹‰ä¼¯æ–‡
        text = "ğŸ˜€ä½ å¥½ã“ã‚“ã«ã¡ã¯ì•ˆë…•Ù…Ø±Ø­Ø¨Ø§"
        seg = ASRDataSeg(text, 0, 1000)
        assert seg.text == text

    def test_empty_translation(self):
        """æµ‹è¯•ç©ºç¿»è¯‘ä¸æ— ç¿»è¯‘çš„åŒºåˆ«"""
        seg1 = ASRDataSeg("Test", 0, 1000)
        seg2 = ASRDataSeg("Test", 0, 1000, translated_text="")
        assert seg1.translated_text == seg2.translated_text == ""

    def test_multiline_text(self):
        """æµ‹è¯•å¤šè¡Œæ–‡æœ¬"""
        text = "Line 1\nLine 2\nLine 3"
        seg = ASRDataSeg(text, 0, 1000)
        assert "\n" in seg.text
        assert seg.text.count("\n") == 2


class TestASRDataEdgeCases:
    """æµ‹è¯• ASRData è¾¹ç¼˜æƒ…å†µ"""

    def test_mixed_empty_and_whitespace(self):
        """æµ‹è¯•æ··åˆç©ºå­—ç¬¦ä¸²å’Œçº¯ç©ºæ ¼"""
        segments = [
            ASRDataSeg("Valid", 0, 1000),
            ASRDataSeg("", 1000, 2000),
            ASRDataSeg("   ", 2000, 3000),
            ASRDataSeg("\t\n", 3000, 4000),
            ASRDataSeg("  Valid  ", 4000, 5000),  # å‰åç©ºæ ¼åº”ä¿ç•™
        ]
        asr_data = ASRData(segments)
        assert len(asr_data) == 2
        assert asr_data.segments[1].text == "  Valid  "

    def test_overlapping_timestamps(self):
        """æµ‹è¯•é‡å çš„æ—¶é—´æˆ³"""
        segments = [
            ASRDataSeg("First", 0, 2000),
            ASRDataSeg("Overlap", 1000, 3000),  # é‡å 
            ASRDataSeg("Third", 2500, 4000),
        ]
        asr_data = ASRData(segments)
        # åº”æŒ‰start_timeæ’åºï¼Œä½†ä¸ä¿®æ­£é‡å 
        assert asr_data.segments[0].text == "First"
        assert asr_data.segments[1].text == "Overlap"

    def test_unsorted_large_dataset(self):
        """æµ‹è¯•å¤§é‡ä¹±åºæ•°æ®"""
        segments = [
            ASRDataSeg(f"Text{i}", i * 1000, (i + 1) * 1000) for i in range(1000, 0, -1)
        ]
        asr_data = ASRData(segments)
        # åº”è¯¥æ­£ç¡®æ’åº
        for i in range(len(asr_data) - 1):
            assert (
                asr_data.segments[i].start_time <= asr_data.segments[i + 1].start_time
            )

    def test_duplicate_timestamps(self):
        """æµ‹è¯•å®Œå…¨ç›¸åŒçš„æ—¶é—´æˆ³"""
        segments = [
            ASRDataSeg("First", 1000, 2000),
            ASRDataSeg("Second", 1000, 2000),
            ASRDataSeg("Third", 1000, 2000),
        ]
        asr_data = ASRData(segments)
        assert len(asr_data) == 3  # éƒ½åº”ä¿ç•™

    def test_single_segment(self):
        """æµ‹è¯•å•ä¸ªå­—å¹•æ®µçš„è¾¹ç•Œæƒ…å†µ"""
        segments = [ASRDataSeg("Only", 0, 1000)]
        asr_data = ASRData(segments)
        # å„ç§æ“ä½œä¸åº”å´©æºƒ
        asr_data.optimize_timing()
        assert len(asr_data) == 1


class TestWordTimestampEdgeCases:
    """æµ‹è¯•è¯çº§æ—¶é—´æˆ³æ£€æµ‹è¾¹ç¼˜æƒ…å†µ"""

    def test_exactly_80_percent_threshold(self):
        """æµ‹è¯•æ°å¥½80%é˜ˆå€¼"""
        # 10ä¸ªç‰‡æ®µï¼Œ8ä¸ªè¯çº§ï¼Œ2ä¸ªå¥å­çº§
        segments = [ASRDataSeg(f"word{i}", i * 100, (i + 1) * 100) for i in range(8)]
        segments.extend(
            [
                ASRDataSeg("This is sentence", 800, 900),
                ASRDataSeg("Another sentence", 900, 1000),
            ]
        )
        asr_data = ASRData(segments)
        assert asr_data.is_word_timestamp()  # 80% åº”è¯¥é€šè¿‡

    def test_79_percent_below_threshold(self):
        """æµ‹è¯•ç•¥ä½äº80%é˜ˆå€¼"""
        # 10ä¸ªç‰‡æ®µï¼Œ7ä¸ªè¯çº§ï¼Œ3ä¸ªå¥å­çº§
        segments = [ASRDataSeg(f"word{i}", i * 100, (i + 1) * 100) for i in range(7)]
        segments.extend(
            [
                ASRDataSeg("This is sentence", 700, 800),
                ASRDataSeg("Another sentence", 800, 900),
                ASRDataSeg("Third sentence", 900, 1000),
            ]
        )
        asr_data = ASRData(segments)
        assert not asr_data.is_word_timestamp()  # 70% ä¸åº”é€šè¿‡

    def test_mixed_cjk_latin_single_chars(self):
        """æµ‹è¯•æ··åˆCJKå’Œæ‹‰ä¸å•å­—ç¬¦"""
        segments = [
            ASRDataSeg("ä½ ", 0, 100),  # CJKå•å­—
            ASRDataSeg("å¥½", 100, 200),
            ASRDataSeg("a", 200, 300),  # æ‹‰ä¸å•å­—ç¬¦
            ASRDataSeg("b", 300, 400),
        ]
        asr_data = ASRData(segments)
        assert asr_data.is_word_timestamp()

    def test_three_char_cjk(self):
        """æµ‹è¯•3å­—ç¬¦CJK(è¾¹ç•Œæƒ…å†µ)"""
        segments = [ASRDataSeg("ä½ å¥½å—", 0, 1000)]  # 3ä¸ªå­—ç¬¦ï¼Œä¸æ˜¯è¯çº§
        asr_data = ASRData(segments)
        assert not asr_data.is_word_timestamp()


class TestSplitToWordsEdgeCases:
    """æµ‹è¯•åˆ†è¯è¾¹ç¼˜æƒ…å†µ"""

    def test_split_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬åˆ†è¯"""
        segments = [ASRDataSeg("", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        assert len(asr_data.segments) == 0

    def test_split_only_punctuation(self):
        """æµ‹è¯•çº¯æ ‡ç‚¹åˆ†è¯"""
        segments = [ASRDataSeg("..., !!!", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        assert len(asr_data.segments) == 0  # æ ‡ç‚¹ä¸åº”åŒ¹é…

    def test_split_very_long_word(self):
        """æµ‹è¯•è¶…é•¿å•è¯"""
        long_word = "a" * 1000
        segments = [ASRDataSeg(long_word, 0, 10000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        assert len(asr_data.segments) == 1
        assert asr_data.segments[0].text == long_word

    def test_split_mixed_scripts(self):
        """æµ‹è¯•æ··åˆå¤šç§æ–‡å­—ç³»ç»Ÿ"""
        # æ‹‰ä¸+ä¸­æ–‡+æ—¥æ–‡+éŸ©æ–‡+é˜¿æ‹‰ä¼¯æ–‡+ä¿„æ–‡
        text = "Helloä½ å¥½ã“ã‚“ã«ã¡ã¯ì•ˆë…•Ù…Ø±Ø­Ø¨Ø§ĞŸÑ€Ğ¸Ğ²ĞµÑ‚"
        segments = [ASRDataSeg(text, 0, 7000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        # åº”è¯¥æ­£ç¡®åˆ†å‰²å„ç§æ–‡å­—
        assert len(asr_data.segments) > 5
        texts = [seg.text for seg in asr_data.segments]
        assert "Hello" in texts
        assert "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚" in texts

    def test_split_numbers_and_words(self):
        """æµ‹è¯•æ•°å­—å’Œå•è¯æ··åˆ"""
        segments = [ASRDataSeg("version 3.14 build 2024", 0, 3000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        texts = [seg.text for seg in asr_data.segments]
        assert "version" in texts
        assert "3" in texts or "14" in texts  # æ•°å­—åº”è¢«åˆ†å¼€
        assert "build" in texts
        assert "2024" in texts

    def test_split_thai_with_combining_chars(self):
        """æµ‹è¯•æ³°æ–‡å¸¦ç»„åˆå­—ç¬¦"""
        thai_text = "à¸ªà¸§à¸±à¸ªà¸”à¸µ"  # æ³°æ–‡ "ä½ å¥½"
        segments = [ASRDataSeg(thai_text, 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        assert len(asr_data.segments) > 0  # åº”è¯¥èƒ½åŒ¹é…æ³°æ–‡

    def test_split_zero_duration_distribution(self):
        """æµ‹è¯•é›¶æ—¶é•¿çš„æ—¶é—´åˆ†é…"""
        segments = [ASRDataSeg("Hello world", 1000, 1000)]
        asr_data = ASRData(segments)
        asr_data.split_to_word_segments()
        # é›¶æ—¶é•¿åº”è¯¥ä¸å´©æºƒ
        assert all(seg.start_time == 1000 for seg in asr_data.segments)
        assert all(seg.end_time == 1000 for seg in asr_data.segments)


class TestMergeEdgeCases:
    """æµ‹è¯•åˆå¹¶è¾¹ç¼˜æƒ…å†µ"""

    def test_merge_single_segment(self):
        """æµ‹è¯•åˆå¹¶å•ä¸ªç‰‡æ®µ(è‡ªå·±å’Œè‡ªå·±)"""
        segments = [ASRDataSeg("Only", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.merge_segments(0, 0)
        assert len(asr_data.segments) == 1
        assert asr_data.segments[0].text == "Only"

    def test_merge_all_segments(self):
        """æµ‹è¯•åˆå¹¶æ‰€æœ‰ç‰‡æ®µ"""
        segments = [ASRDataSeg(f"T{i}", i * 100, (i + 1) * 100) for i in range(10)]
        asr_data = ASRData(segments)
        asr_data.merge_segments(0, 9)
        assert len(asr_data.segments) == 1
        assert "T0" in asr_data.segments[0].text
        assert "T9" in asr_data.segments[0].text

    def test_merge_invalid_indices(self):
        """æµ‹è¯•æ— æ•ˆçš„åˆå¹¶ç´¢å¼•"""
        segments = [ASRDataSeg("A", 0, 1000), ASRDataSeg("B", 1000, 2000)]
        asr_data = ASRData(segments)

        with pytest.raises(IndexError):
            asr_data.merge_segments(-1, 1)  # è´Ÿç´¢å¼•
        with pytest.raises(IndexError):
            asr_data.merge_segments(0, 5)  # è¶…å‡ºèŒƒå›´
        with pytest.raises(IndexError):
            asr_data.merge_segments(1, 0)  # start > end

    def test_merge_with_next_at_boundary(self):
        """æµ‹è¯•åœ¨è¾¹ç•Œä½ç½®åˆå¹¶"""
        segments = [ASRDataSeg("Only", 0, 1000)]
        asr_data = ASRData(segments)

        with pytest.raises(IndexError):
            asr_data.merge_with_next_segment(0)  # æ²¡æœ‰ä¸‹ä¸€ä¸ª

    def test_merge_with_unicode(self):
        """æµ‹è¯•åˆå¹¶Unicodeæ–‡æœ¬"""
        segments = [
            ASRDataSeg("ğŸ˜€ä½ å¥½", 0, 1000),
            ASRDataSeg("ğŸŒworld", 1000, 2000),
        ]
        asr_data = ASRData(segments)
        asr_data.merge_with_next_segment(0)
        assert "ğŸ˜€" in asr_data.segments[0].text
        assert "ğŸŒ" in asr_data.segments[0].text


class TestOptimizeTimingEdgeCases:
    """æµ‹è¯•æ—¶é—´ä¼˜åŒ–è¾¹ç¼˜æƒ…å†µ"""

    def test_optimize_negative_gap(self):
        """æµ‹è¯•è´Ÿé—´éš”(é‡å )"""
        segments = [
            ASRDataSeg("First", 0, 2000),
            ASRDataSeg("Overlap", 1500, 3000),  # é‡å 500ms
        ]
        asr_data = ASRData(segments)
        asr_data.optimize_timing()
        # è´Ÿé—´éš”ä¸åº”ä¼˜åŒ–(æˆ–æ ¹æ®å®ç°è°ƒæ•´)
        assert asr_data.segments[0].end_time == 2000

    def test_optimize_exact_threshold(self):
        """æµ‹è¯•æ°å¥½åœ¨é˜ˆå€¼è¾¹ç•Œ"""
        segments = [
            ASRDataSeg("First sentence", 0, 1000),
            ASRDataSeg("Second sentence", 2000, 3000),  # æ°å¥½1000ms gap
        ]
        asr_data = ASRData(segments)
        asr_data.optimize_timing(threshold_ms=1000)
        # æ°å¥½ç­‰äºé˜ˆå€¼ä¸ä¼˜åŒ–(éœ€è¦ < threshold)
        gap = asr_data.segments[1].start_time - asr_data.segments[0].end_time
        assert gap == 1000  # åº”è¯¥ä¿æŒä¸å˜

    def test_optimize_word_level_no_change(self):
        """æµ‹è¯•è¯çº§æ—¶é—´æˆ³ä¸ä¼˜åŒ–"""
        segments = [
            ASRDataSeg("Word1", 0, 500),
            ASRDataSeg("Word2", 1000, 1500),
        ]
        asr_data = ASRData(segments)
        original_end = asr_data.segments[0].end_time

        asr_data.optimize_timing()
        # è¯çº§åº”è¯¥è·³è¿‡ä¼˜åŒ–
        assert asr_data.segments[0].end_time == original_end


class TestRemovePunctuationEdgeCases:
    """æµ‹è¯•ç§»é™¤æ ‡ç‚¹è¾¹ç¼˜æƒ…å†µ"""

    def test_remove_multiple_punctuation(self):
        """æµ‹è¯•è¿ç»­å¤šä¸ªæ ‡ç‚¹"""
        segments = [ASRDataSeg("ä½ å¥½ï¼Œï¼Œï¼Œã€‚ã€‚ã€‚", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.remove_punctuation()
        assert asr_data.segments[0].text == "ä½ å¥½"

    def test_remove_punctuation_only(self):
        """æµ‹è¯•çº¯æ ‡ç‚¹æ–‡æœ¬"""
        segments = [ASRDataSeg("ï¼Œã€‚ï¼Œã€‚", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.remove_punctuation()
        assert asr_data.segments[0].text == ""

    def test_remove_punctuation_middle(self):
        """æµ‹è¯•ä¸­é—´çš„æ ‡ç‚¹ä¸ç§»é™¤"""
        segments = [ASRDataSeg("ä½ å¥½ï¼Œä¸–ç•Œã€‚", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.remove_punctuation()
        assert asr_data.segments[0].text == "ä½ å¥½ï¼Œä¸–ç•Œ"  # åªåˆ å°¾éƒ¨

    def test_remove_non_chinese_punctuation(self):
        """æµ‹è¯•éä¸­æ–‡æ ‡ç‚¹ä¸ç§»é™¤"""
        segments = [ASRDataSeg("Hello, world!", 0, 1000)]
        asr_data = ASRData(segments)
        asr_data.remove_punctuation()
        assert asr_data.segments[0].text == "Hello, world!"  # ä¸å˜


class TestFormatConversionEdgeCases:
    """æµ‹è¯•æ ¼å¼è½¬æ¢è¾¹ç¼˜æƒ…å†µ"""

    def test_srt_layout_modes_all(self):
        """æµ‹è¯•æ‰€æœ‰SRTå¸ƒå±€æ¨¡å¼"""
        from app.core.entities import SubtitleLayoutEnum

        segments = [ASRDataSeg("Hello", 0, 1000, translated_text="ä½ å¥½")]
        asr_data = ASRData(segments)

        srt1 = asr_data.to_srt(layout=SubtitleLayoutEnum.ORIGINAL_ON_TOP)
        assert "Hello\nä½ å¥½" in srt1

        srt2 = asr_data.to_srt(layout=SubtitleLayoutEnum.TRANSLATE_ON_TOP)
        assert "ä½ å¥½\nHello" in srt2

        srt3 = asr_data.to_srt(layout=SubtitleLayoutEnum.ONLY_ORIGINAL)
        assert "Hello" in srt3
        assert "ä½ å¥½" not in srt3

        srt4 = asr_data.to_srt(layout=SubtitleLayoutEnum.ONLY_TRANSLATE)
        assert "ä½ å¥½" in srt4

    def test_srt_no_translation_all_layouts(self):
        """æµ‹è¯•æ— ç¿»è¯‘æ—¶çš„æ‰€æœ‰å¸ƒå±€"""
        segments = [ASRDataSeg("Hello", 0, 1000)]
        asr_data = ASRData(segments)

        for layout in ["åŸæ–‡åœ¨ä¸Š", "è¯‘æ–‡åœ¨ä¸Š", "ä»…åŸæ–‡", "ä»…è¯‘æ–‡"]:
            srt = asr_data.to_srt(layout=layout)
            assert "Hello" in srt  # æ‰€æœ‰æ¨¡å¼éƒ½åº”æ˜¾ç¤ºåŸæ–‡

    def test_json_large_dataset(self):
        """æµ‹è¯•å¤§æ•°æ®é›†JSONè½¬æ¢"""
        segments = [
            ASRDataSeg(f"Text{i}", i * 1000, (i + 1) * 1000) for i in range(1000)
        ]
        asr_data = ASRData(segments)
        json_data = asr_data.to_json()
        assert len(json_data) == 1000
        assert "1" in json_data
        assert "1000" in json_data

    def test_txt_multiline_segments(self):
        """æµ‹è¯•å¤šè¡Œæ–‡æœ¬è½¬æ¢"""
        segments = [
            ASRDataSeg("Line1\nLine2", 0, 1000),
            ASRDataSeg("Line3", 1000, 2000),
        ]
        asr_data = ASRData(segments)
        txt = asr_data.to_txt()
        assert "Line1\nLine2" in txt


class TestFileIOEdgeCases:
    """æµ‹è¯•æ–‡ä»¶è¯»å†™è¾¹ç¼˜æƒ…å†µ"""

    def test_save_unsupported_format(self):
        """æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼"""
        segments = [ASRDataSeg("Test", 0, 1000)]
        asr_data = ASRData(segments)

        with tempfile.NamedTemporaryFile(suffix=".xyz", delete=False) as f:
            temp_path = f.name

        try:
            with pytest.raises(ValueError, match="Unsupported file extension"):
                asr_data.save(temp_path)
        finally:
            Path(temp_path).unlink(missing_ok=True)

    def test_load_nonexistent_file(self):
        """æµ‹è¯•åŠ è½½ä¸å­˜åœ¨çš„æ–‡ä»¶"""
        with pytest.raises(FileNotFoundError):
            ASRData.from_subtitle_file("/nonexistent/path/file.srt")

    def test_save_load_unicode_path(self):
        """æµ‹è¯•Unicodeæ–‡ä»¶è·¯å¾„"""
        segments = [ASRDataSeg("æµ‹è¯•", 0, 1000)]
        asr_data = ASRData(segments)

        with tempfile.TemporaryDirectory() as tmpdir:
            unicode_path = Path(tmpdir) / "æµ‹è¯•æ–‡ä»¶å.srt"
            asr_data.save(str(unicode_path))
            loaded = ASRData.from_subtitle_file(str(unicode_path))
            assert loaded.segments[0].text == "æµ‹è¯•"


class TestParseEdgeCases:
    """æµ‹è¯•è§£æè¾¹ç¼˜æƒ…å†µ"""

    def test_parse_malformed_srt(self):
        """æµ‹è¯•ç•¸å½¢SRT"""
        malformed = """1
00:00:00,000 --> INVALID
Hello

2
INVALID TIMESTAMP
World
"""
        asr_data = ASRData.from_srt(malformed)
        assert len(asr_data.segments) == 0  # åº”è·³è¿‡æ— æ•ˆå—

    def test_parse_srt_missing_text(self):
        """æµ‹è¯•ç¼ºå°‘æ–‡æœ¬çš„SRTå—"""
        srt = """1
00:00:00,000 --> 00:00:01,000

2
00:00:01,000 --> 00:00:02,000
Valid
"""
        asr_data = ASRData.from_srt(srt)
        assert len(asr_data.segments) == 1
        assert asr_data.segments[0].text == "Valid"

    def test_parse_srt_97_percent_translation(self):
        """æµ‹è¯•97%ç¿»è¯‘(ä½äº98%é˜ˆå€¼)"""
        # 100ä¸ªå—ï¼Œ97ä¸ªæœ‰ç¿»è¯‘
        blocks = []
        for i in range(97):
            blocks.append(
                f"{i+1}\n00:00:{i:02d},000 --> 00:00:{i+1:02d},000\nText{i}\nTrans{i}\n"
            )
        for i in range(97, 100):
            blocks.append(
                f"{i+1}\n00:00:{i:02d},000 --> 00:00:{i+1:02d},000\nText{i}\n"
            )

        srt = "\n".join(blocks)
        asr_data = ASRData.from_srt(srt)
        # ä½äº98%ä¸åº”è¯†åˆ«ä¸ºç¿»è¯‘æ ¼å¼
        assert not asr_data.segments[0].translated_text

    def test_parse_json_non_numeric_keys(self):
        """æµ‹è¯•JSONéæ•°å­—é”®"""
        json_data = {
            "a": {
                "original_subtitle": "Test",
                "translated_subtitle": "",
                "start_time": 0,
                "end_time": 1000,
            }
        }
        with pytest.raises(ValueError):
            ASRData.from_json(json_data)

    def test_parse_vtt_empty_blocks(self):
        """æµ‹è¯•VTTç©ºå—"""
        vtt = """WEBVTT

HEADER


1
00:00:01.000 --> 00:00:02.000
Text1


"""
        asr_data = ASRData.from_vtt(vtt)
        assert len(asr_data.segments) == 1
